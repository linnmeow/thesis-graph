{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Pclanglais/Brahe-Novels\")\n",
    "\n",
    "# print(dataset.keys())\n",
    "\n",
    "# only train dataset is available\n",
    "train_dataset = dataset['train']\n",
    "# validation_dataset = dataset['validation']\n",
    "# test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8226\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "# print(f\"Number of validation samples: {len(validation_dataset)}\")\n",
    "# print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/lynn/Desktop/thesis/graph/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "Using legacy 'setup.py install' for langdetect, since package 'wheel' is not installed.\n",
      "Installing collected packages: langdetect\n",
      "    Running setup.py install for langdetect ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/lynn/Desktop/thesis/graph/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering out the non English texts: 4302\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from transformers import BartTokenizer\n",
    "from datasets import load_dataset\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "def save_to_disk(data, filename):\n",
    "    \"\"\"Save the data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def tokenize_text(tokenizer, text):\n",
    "    \"\"\"Tokenize the text using the BART tokenizer.\"\"\"\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "def count_tokens(tokenizer, text):\n",
    "    \"\"\"Count tokens in the text using the BART tokenizer.\"\"\"\n",
    "    return len(tokenize_text(tokenizer, text))\n",
    "\n",
    "def filter_long_texts(dataset, tokenizer, max_tokens):\n",
    "    \"\"\"Filter out examples where the text length exceeds max_tokens.\"\"\"\n",
    "    filtered_examples = []\n",
    "    for example in dataset:\n",
    "        if count_tokens(tokenizer, example['full_text']) <= max_tokens:\n",
    "            filtered_examples.append(example)\n",
    "    return filtered_examples\n",
    "\n",
    "def filter_non_english(dataset):\n",
    "    \"\"\"Filter out non-English examples based on language detection.\"\"\"\n",
    "    english_examples = []\n",
    "    for example in dataset:\n",
    "        try:\n",
    "            # Detect the language of the article\n",
    "            if detect(example['full_text']) == 'en':\n",
    "                english_examples.append(example)\n",
    "        except:\n",
    "            # Skip examples where language detection fails\n",
    "            continue\n",
    "    return english_examples\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# # filter out texts longer than 1050 tokens\n",
    "# filtered_dataset = filter_long_texts(train_dataset, tokenizer, 1024)\n",
    "# print(f\"After filtering out the long texts: {len(filtered_dataset)}\")\n",
    "\n",
    "filtered_dataset = filter_non_english(train_dataset)\n",
    "print(f\"After filtering out the non English texts: {len(filtered_dataset)}\")\n",
    "\n",
    "\n",
    "# save the selected examples to disk\n",
    "save_to_disk(filtered_dataset, 'brahe.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to discard unwanted data from analysis\n",
    "def transform_data(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    transformed_data = []\n",
    "    for entry in data:\n",
    "        summary = entry.get('analysis', '').split('\\n')[0].replace('Summary: ', '')\n",
    "        \n",
    "        new_entry = {\n",
    "            'id': entry.get('instruction_id', ''),\n",
    "            'document': entry.get('full_text', ''),\n",
    "            'summary': summary,\n",
    "        }\n",
    "        \n",
    "        transformed_data.append(new_entry)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(transformed_data, f, indent=4)\n",
    "\n",
    "input_file = 'brahe.json'\n",
    "output_file = 'transformed_brahe.json'  \n",
    "transform_data(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coverage: 0.54\n",
      "Average Density: 1.95\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def compute_fragments(source_text, summary_text):\n",
    "    # tokenize the source document and summary\n",
    "    source_tokens = word_tokenize(source_text.lower())\n",
    "    summary_tokens = word_tokenize(summary_text.lower())\n",
    "    \n",
    "    # create token-to-index mappings for quick lookup\n",
    "    source_token_indices = {token: idx for idx, token in enumerate(source_tokens)}\n",
    "    \n",
    "    # initialize fragment length lists\n",
    "    fragment_lengths = []\n",
    "    current_fragment_length = 0\n",
    "    \n",
    "    for token in summary_tokens:\n",
    "        if token in source_token_indices:\n",
    "            if current_fragment_length == 0:\n",
    "                current_fragment_length = 1\n",
    "            else:\n",
    "                current_fragment_length += 1\n",
    "        else:\n",
    "            if current_fragment_length > 0:\n",
    "                fragment_lengths.append(current_fragment_length)\n",
    "                current_fragment_length = 0\n",
    "    \n",
    "    if current_fragment_length > 0:\n",
    "        fragment_lengths.append(current_fragment_length)\n",
    "    \n",
    "    return fragment_lengths\n",
    "\n",
    "def compute_coverage_and_density(source_text, summary_text):\n",
    "    fragment_lengths = compute_fragments(source_text, summary_text)\n",
    "    \n",
    "    # total number of words in the summary\n",
    "    summary_word_count = len(word_tokenize(summary_text.lower()))\n",
    "    \n",
    "    # coverage: Percentage of words in the summary that are part of an extractive fragment\n",
    "    coverage = sum(fragment_lengths) / summary_word_count if summary_word_count > 0 else 0\n",
    "    \n",
    "    # density: Average length of the extractive fragment squared\n",
    "    density = (sum(length ** 2 for length in fragment_lengths) / summary_word_count\n",
    "               if summary_word_count > 0 else 0)\n",
    "    \n",
    "    return coverage, density\n",
    "\n",
    "def calculate_average_coverage_and_density(json_file_path):\n",
    "    coverage_list = []\n",
    "    density_list = []\n",
    "    \n",
    "    # load the dataset from the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "    \n",
    "    # process each instance in the dataset\n",
    "    for instance in dataset:\n",
    "        source_text = instance.get('text', '')\n",
    "        summary_text = instance.get('summary', '')\n",
    "\n",
    "        # source_text = instance.get('article', '')\n",
    "        # summary_text = instance.get('highlights', '')\n",
    "        \n",
    "        coverage, density = compute_coverage_and_density(source_text, summary_text)\n",
    "        \n",
    "        coverage_list.append(coverage)\n",
    "        density_list.append(density)\n",
    "    \n",
    "    # calculate average coverage and density\n",
    "    avg_coverage = sum(coverage_list) / len(coverage_list) if coverage_list else 0\n",
    "    avg_density = sum(density_list) / len(density_list) if density_list else 0\n",
    "    \n",
    "    return avg_coverage, avg_density\n",
    "\n",
    "average_coverage, average_density = calculate_average_coverage_and_density('/Users/lynn/Desktop/thesis/booksum_paragraph-level-summary-alignments/article_collections/train_article.json')\n",
    "print(f\"Average Coverage: {average_coverage:.2f}\")\n",
    "print(f\"Average Density: {average_density:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in document: 163.11\n",
      "Average number of tokens in summary: 33.49\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "def calculate_average_tokens(json_file_path):\n",
    "    \"\"\"\n",
    "    Calculate the average number of tokens in the document and summary fields of a JSON dataset.\n",
    "    \n",
    "    Args:\n",
    "    - json_file_path (str): Path to the JSON file containing the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: (average_tokens_document, average_tokens_summary)\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    doc_lengths = []\n",
    "    sum_lengths = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Split the document and summary into tokens\n",
    "        doc_tokens = entry.get('document', '').split()\n",
    "        sum_tokens = entry.get('summary', '').split()\n",
    "        \n",
    "        doc_lengths.append(len(doc_tokens))\n",
    "        sum_lengths.append(len(sum_tokens))\n",
    "\n",
    "    avg_tokens_document = mean(doc_lengths) if doc_lengths else 0\n",
    "    avg_tokens_summary = mean(sum_lengths) if sum_lengths else 0\n",
    "    \n",
    "    return avg_tokens_document, avg_tokens_summary\n",
    "\n",
    "avg_doc_tokens, avg_sum_tokens = calculate_average_tokens('transformed_brahe.json')\n",
    "print(f'Average number of tokens in document: {avg_doc_tokens:.2f}')\n",
    "print(f'Average number of tokens in summary: {avg_sum_tokens:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset from JSON file\n",
    "with open('selected_examples.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "total_examples = len(data)\n",
    "train_size = 8000 / total_examples\n",
    "test_size = 1000 / total_examples\n",
    "valid_size = 1000 / total_examples\n",
    "\n",
    "# split the data into training, validation, and test sets\n",
    "train_data, remaining_data = train_test_split(data, test_size=(test_size + valid_size), random_state=42)\n",
    "test_data, valid_data = train_test_split(remaining_data, test_size=(valid_size / (test_size + valid_size)), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data to JSON file\n",
    "with open('train_article.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file)\n",
    "\n",
    "# save testing data to JSON file\n",
    "with open('test_article.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file)\n",
    "\n",
    "# save validation data to JSON file\n",
    "with open('valid_article.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentEncoder(nn.Module):\n",
    "    def __init__(self, roberta_model_name, hidden_size):\n",
    "        super(DocumentEncoder, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
    "        self.bilstm = nn.LSTM(self.roberta.config.hidden_size, \n",
    "                              hidden_size // 2,  # BiLSTM has half the hidden size per direction\n",
    "                              num_layers=1,\n",
    "                              bidirectional=True)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Tokenize input text\n",
    "        tokens = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = tokens.input_ids\n",
    "        attention_mask = tokens.attention_mask\n",
    "        \n",
    "        # Pass input through RoBERTa\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Pass RoBERTa outputs through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(last_hidden_states)\n",
    "        \n",
    "        return lstm_output, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummaryDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(SummaryDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=1)\n",
    "        self.attention_document = nn.Linear(hidden_size, hidden_size)\n",
    "        self.u1 = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.Wout = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden_state, document_output, document_mask):\n",
    "        # LSTM step\n",
    "        lstm_output, hidden_state = self.lstm(input_token.unsqueeze(0), hidden_state)\n",
    "\n",
    "        # Attention over document\n",
    "        document_scores = torch.tanh(self.attention_document(lstm_output) + document_output)\n",
    "        document_scores = torch.matmul(document_scores, self.u1)\n",
    "        document_scores = document_scores.masked_fill(~document_mask, float('-inf'))\n",
    "        document_attention_weights = F.softmax(document_scores, dim=1)\n",
    "        document_context_vector = torch.sum(document_attention_weights * document_output, dim=1)\n",
    "\n",
    "        # Combine context vector with LSTM output\n",
    "        combined_context = torch.cat((lstm_output.squeeze(0), document_context_vector), dim=1)\n",
    "        output = self.Wout(combined_context)\n",
    "\n",
    "        return output, hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, document_text, target_summary=None, teacher_forcing_ratio=0.5):\n",
    "        # Encode the document\n",
    "        document_output, document_mask = self.encoder(document_text)\n",
    "        document_output = document_output.to(self.device)\n",
    "        document_mask = document_mask.to(self.device)\n",
    "\n",
    "        # Prepare initial decoder input and hidden state\n",
    "        batch_size = document_output.size(0)\n",
    "        decoder_input = torch.zeros((batch_size, self.decoder.hidden_size)).to(self.device)\n",
    "        hidden_state = None  # LSTM hidden state will be initialized to zero by default\n",
    "\n",
    "        # Iterate over the target sequence\n",
    "        outputs = []\n",
    "        for t in range(target_summary.size(1)):\n",
    "            output, hidden_state = self.decoder(decoder_input, hidden_state, document_output, document_mask)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "            # Teacher forcing: use actual target token or predicted token as next input\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            decoder_input = target_summary[:, t, :] if teacher_force else output\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "    def generate_summary(self, document_text, max_length=50):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Encode the document\n",
    "            document_output, document_mask = self.encoder(document_text)\n",
    "            document_output = document_output.to(self.device)\n",
    "            document_mask = document_mask.to(self.device)\n",
    "\n",
    "            # Prepare initial decoder input and hidden state\n",
    "            batch_size = document_output.size(0)\n",
    "            decoder_input = torch.zeros((batch_size, self.decoder.hidden_size)).to(self.device)\n",
    "            hidden_state = None  # LSTM hidden state will be initialized to zero by default\n",
    "\n",
    "            # Generate summary tokens\n",
    "            summary_tokens = []\n",
    "            for _ in range(max_length):\n",
    "                output, hidden_state = self.decoder(decoder_input, hidden_state, document_output, document_mask)\n",
    "                summary_tokens.append(output.argmax(dim=1).unsqueeze(1))\n",
    "\n",
    "                # Next input is the current output\n",
    "                decoder_input = output\n",
    "\n",
    "            summary_tokens = torch.cat(summary_tokens, dim=1)\n",
    "        return summary_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 768\n",
    "output_size = len(RobertaTokenizer.from_pretrained('roberta-base').vocab)\n",
    "\n",
    "encoder = DocumentEncoder('roberta-base', hidden_size).to(device)\n",
    "decoder = SummaryDecoder(hidden_size, output_size).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=RobertaTokenizer.from_pretrained('roberta-base').pad_token_id)\n",
    "\n",
    "# Training loop (example, assuming `dataloader` provides batches of document texts and target summaries)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        document_text, target_summary = batch\n",
    "        document_text, target_summary = document_text.to(device), target_summary.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(document_text, target_summary)\n",
    "        loss = criterion(output.view(-1, output_size), target_summary.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, generate a summary (example)\n",
    "model.eval()\n",
    "test_input_text = [\"Your test input text here...\"]\n",
    "summary_tokens = model.generate_summary(test_input_text)\n",
    "summary_text = RobertaTokenizer.from_pretrained('roberta-base').decode(summary_tokens[0], skip_special_tokens=True)\n",
    "print(f'Generated Summary: {summary_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Graph Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummaryDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(SummaryDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=1)\n",
    "        self.attention_graph = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_document = nn.Linear(hidden_size, hidden_size)\n",
    "        self.u0 = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.u1 = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.Wout = nn.Linear(hidden_size * 3, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden_state, document_output, graph_output, document_mask, graph_mask):\n",
    "        # LSTM step\n",
    "        lstm_output, hidden_state = self.lstm(input_token.unsqueeze(0), hidden_state)\n",
    "\n",
    "        # Attention over graph\n",
    "        graph_scores = torch.tanh(self.attention_graph(lstm_output) + graph_output)\n",
    "        graph_scores = torch.matmul(graph_scores, self.u0)\n",
    "        graph_scores = graph_scores.masked_fill(~graph_mask, float('-inf'))\n",
    "        graph_attention_weights = F.softmax(graph_scores, dim=1)\n",
    "        graph_context_vector = torch.sum(graph_attention_weights * graph_output, dim=1)\n",
    "\n",
    "        # Attention over document\n",
    "        document_scores = torch.tanh(self.attention_document(lstm_output) + document_output + graph_context_vector.unsqueeze(1))\n",
    "        document_scores = torch.matmul(document_scores, self.u1)\n",
    "        document_scores = document_scores.masked_fill(~document_mask, float('-inf'))\n",
    "        document_attention_weights = F.softmax(document_scores, dim=1)\n",
    "        document_context_vector = torch.sum(document_attention_weights * document_output, dim=1)\n",
    "\n",
    "        # Combine context vectors with LSTM output\n",
    "        combined_context = torch.cat((lstm_output.squeeze(0), document_context_vector, graph_context_vector), dim=1)\n",
    "        output = self.Wout(combined_context)\n",
    "\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, document_text, graph_nodes, graph_mask, target_summary=None, teacher_forcing_ratio=0.5):\n",
    "        # Encode the document\n",
    "        document_output, document_mask = self.encoder(document_text)\n",
    "        document_output = document_output.to(self.device)\n",
    "        document_mask = document_mask.to(self.device)\n",
    "\n",
    "        # Encode the graph (assuming graph_nodes is already processed and is a tensor)\n",
    "        graph_output = graph_nodes.to(self.device)\n",
    "        graph_mask = graph_mask.to(self.device)\n",
    "\n",
    "        # Prepare initial decoder input and hidden state\n",
    "        batch_size = document_output.size(0)\n",
    "        decoder_input = torch.zeros((batch_size, self.decoder.hidden_size)).to(self.device)\n",
    "        hidden_state = None  # LSTM hidden state will be initialized to zero by default\n",
    "                # Prepare initial decoder input and hidden state\n",
    "        batch_size = document_output.size(0)\n",
    "        decoder_input = torch.zeros((batch_size, self.decoder.hidden_size)).to(self.device)\n",
    "        hidden_state = None  # LSTM hidden state will be initialized to zero by default\n",
    "\n",
    "        # Iterate over the target sequence\n",
    "        outputs = []\n",
    "        for t in range(target_summary.size(1)):\n",
    "            output, hidden_state = self.decoder(decoder_input, hidden_state, document_output, graph_output, document_mask, graph_mask)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "            # Teacher forcing: use actual target token or predicted token as next input\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            decoder_input = target_summary[:, t, :] if teacher_force else output\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(self, document_text, graph_nodes, graph_mask, max_length=50):\n",
    "    self.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the document\n",
    "        document_output, document_mask = self.encoder(document_text)\n",
    "        document_output = document_output.to(self.device)\n",
    "        document_mask = document_mask.to(self.device)\n",
    "\n",
    "        # Encode the graph (assuming graph_nodes is already processed and is a tensor)\n",
    "        graph_output = graph_nodes.to(self.device)\n",
    "        graph_mask = graph_mask.to(self.device)\n",
    "\n",
    "        # Prepare initial decoder input and hidden state\n",
    "        batch_size = document_output.size(0)\n",
    "        decoder_input = torch.zeros((batch_size, self.decoder.hidden_size)).to(self.device)\n",
    "        hidden_state = None  # LSTM hidden state will be initialized to zero by default\n",
    "\n",
    "        # Generate summary tokens\n",
    "        summary_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            output, hidden_state = self.decoder(decoder_input, hidden_state, document_output, graph_output, document_mask, graph_mask)\n",
    "            summary_tokens.append(output.argmax(dim=1).unsqueeze(1))\n",
    "\n",
    "            # Next input is the current output\n",
    "            decoder_input = output\n",
    "\n",
    "        summary_tokens = torch.cat(summary_tokens, dim=1)\n",
    "    return summary_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 768\n",
    "output_size = len(RobertaTokenizer.from_pretrained('roberta-base').vocab)\n",
    "\n",
    "encoder = DocumentEncoder('roberta-base', hidden_size).to(device)\n",
    "decoder = SummaryDecoder(hidden_size, output_size).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=RobertaTokenizer.from_pretrained('roberta-base').pad_token_id)\n",
    "\n",
    "# Training loop (example, assuming `dataloader` provides batches of document texts, graph nodes, graph masks, and target summaries)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        document_text, graph_nodes, graph_mask, target_summary = batch\n",
    "        document_text, graph_nodes, graph_mask, target_summary = document_text.to(device), graph_nodes.to(device), graph_mask.to(device), target_summary.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(document_text, graph_nodes, graph_mask, target_summary)\n",
    "        loss = criterion(output.view(-1, output_size), target_summary.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, generate a summary (example)\n",
    "model.eval()\n",
    "test_input_text = [\"Your test input text here...\"]\n",
    "test_graph_nodes = torch.randn((1, 10, hidden_size))  # Example graph nodes\n",
    "test_graph_mask = torch.ones((1, 10), dtype=torch.bool)  # Example graph mask\n",
    "summary_tokens = model.generate_summary(test_input_text, test_graph_nodes, test_graph_mask)\n",
    "summary_text = RobertaTokenizer.from_pretrained('roberta-base').decode(summary_tokens[0], skip_special_tokens=True)\n",
    "print(f'Generated Summary: {summary_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def sample_data(data, sample_size):\n",
    "    if len(data) <= sample_size:\n",
    "        return data  \n",
    "    return random.sample(data, sample_size)\n",
    "\n",
    "# remove the 'alignment_scores' key from each dictionary if it exists\n",
    "def remove_key(data, key):\n",
    "    for item in data:\n",
    "        if key in item:\n",
    "            del item[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data saved to fiction.json\n",
      "Total number of examples in the output file: 10000\n"
     ]
    }
   ],
   "source": [
    "data1 = load_json('../brahe/brahe.json')  # file to sample from\n",
    "data2 = load_json('../booksum_paragraph-level-summary-alignments/booksum_article_filtered.json')  # file to concatenate with\n",
    "\n",
    "sampled_data1 = sample_data(data1, 3640)\n",
    "\n",
    "# remove 'alignment_scores' from the sampled data\n",
    "remove_key(data2, 'alignment_scores')\n",
    "\n",
    "concatenated_data = sampled_data1 + data2\n",
    "\n",
    "save_json(concatenated_data, 'fiction.json')\n",
    "print(\"Concatenated data saved to fiction.json\")\n",
    "print(f\"Total number of examples in the output file: {len(concatenated_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split dataset into train valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path: str, train_file: str, valid_file: str, test_file: str, train_ratio: float = 0.8, valid_ratio: float = 0.1):\n",
    "\n",
    "    data = load_json(file_path)\n",
    "\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    total_size = len(data)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    valid_size = int(total_size * valid_ratio)\n",
    "    test_size = total_size - train_size - valid_size \n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    valid_data = data[train_size:train_size + valid_size]\n",
    "    test_data = data[train_size + valid_size:]\n",
    "\n",
    "    save_json(train_data, train_file)\n",
    "    save_json(valid_data, valid_file)\n",
    "    save_json(test_data, test_file)\n",
    "\n",
    "    print(f\"Training data saved to {train_file}\")\n",
    "    print(f\"Validation data saved to {valid_file}\")\n",
    "    print(f\"Test data saved to {test_file}\")\n",
    "    print(f\"Total number of examples: {total_size}\")\n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Validation set size: {len(valid_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to article_collections/train_article.json\n",
      "Validation data saved to article_collections/valid_article.json\n",
      "Test data saved to article_collections/test_article.json\n",
      "Total number of examples: 10000\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Test set size: 1000\n"
     ]
    }
   ],
   "source": [
    "file_path = 'fiction.json'     # file to split\n",
    "train_file = 'article_collections/train_article.json'      \n",
    "valid_file = 'article_collections/valid_article.json'      \n",
    "test_file = 'article_collections/test_article.json'        \n",
    "\n",
    "split_data(file_path, train_file, valid_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
